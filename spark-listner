from pyspark.scheduler import SparkListener, SparkListenerEvent
import logging

Create a logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataprocSparkListener(SparkListener):
    def __init__(self):
        super(DataprocSparkListener, self).__init__()

    def onJobStart(self, jobStart):
        logger.info(f"Job started: {jobStart.jobId}")

    def onJobEnd(self, jobEnd):
        logger.info(f"Job ended: {jobEnd.jobId}")
        logger.info(f"Job result: {jobEnd.jobResult}")

    def onStageCompleted(self, stageCompleted):
        logger.info(f"Stage completed: {stageCompleted.stageInfo.stageId}")
        logger.info(f"Stage attempt ID: {stageCompleted.stageInfo.attemptId}")
        logger.info(f"Stage status: {stageCompleted.stageInfo.status}")

    def onTaskEnd(self, taskEnd):
        logger.info(f"Task ended: {taskEnd.taskId}")
        logger.info(f"Task attempt ID: {taskEnd.taskAttemptId}")
        logger.info(f"Task status: {taskEnd.taskInfo.status}")